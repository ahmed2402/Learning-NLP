{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d280c28e",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "Word2Vec is a popular technique used in natural language processing to turn words into numerical vectors, so that computers can understand and work with text data. These vectors capture the meaning of words in such a way that similar words have similar vectors.\n",
    "\n",
    "### How does Word2Vec work?\n",
    " Word2Vec uses a neural network to learn word relationships from a large amount of text. It does this by looking at the context in which words appear. There are two main ways Word2Vec can be trained: CBOW and Skip-gram.\n",
    "\n",
    "### CBOW (Continuous Bag of Words):\n",
    " In CBOW, the model tries to predict a target word based on the words that come before and after it (the context). For example, given the sentence \"the cat sits on the mat\", if the context is [\"the\", \"cat\", \"on\", \"the\", \"mat\"], the model tries to predict the word \"sits\".\n",
    "\n",
    "### Skip-gram:\n",
    " In Skip-gram, the model does the opposite: it takes a single word and tries to predict the words around it (the context). Using the same sentence, if the target word is \"sits\", the model tries to predict [\"the\", \"cat\", \"on\", \"the\", \"mat\"].\n",
    "\n",
    "- In summary, CBOW predicts a word from its context, while Skip-gram predicts the context from a word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d86490a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\zainab\\anaconda3\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\zainab\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\zainab\\anaconda3\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\zainab\\anaconda3\\lib\\site-packages (from gensim) (1.11.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\zainab\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: click in c:\\users\\zainab\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\zainab\\anaconda3\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\zainab\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\zainab\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\zainab\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: gensim in c:\\users\\zainab\\anaconda3\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\zainab\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\zainab\\anaconda3\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\zainab\\anaconda3\\lib\\site-packages (from gensim) (1.11.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\zainab\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: click in c:\\users\\zainab\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\zainab\\anaconda3\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\zainab\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\zainab\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\zainab\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef7a7ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Zainab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "sentences = [\n",
    "    \"I love natural language processing\",\n",
    "    \"Word embeddings are amazing\",\n",
    "    \"I enjoy learning machine learning concepts\",\n",
    "    \"Deep learning uses neural networks\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 'love', 'natural', 'language', 'processing'],\n",
       " ['word', 'embeddings', 'are', 'amazing'],\n",
       " ['i', 'enjoy', 'learning', 'machine', 'learning', 'concepts'],\n",
       " ['deep', 'learning', 'uses', 'neural', 'networks']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(\n",
    "    sentences=tokenized_sentences,\n",
    "    vector_size=100,    # Dimension of word vectors\n",
    "    window=5,          # Context window size\n",
    "    min_count=1,       # Ignore words with frequency < 1\n",
    "    workers=4          # Parallel processing threads\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e48f8fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learning',\n",
       " 'i',\n",
       " 'are',\n",
       " 'love',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'word',\n",
       " 'embeddings',\n",
       " 'networks',\n",
       " 'neural',\n",
       " 'enjoy',\n",
       " 'machine',\n",
       " 'concepts',\n",
       " 'deep',\n",
       " 'uses',\n",
       " 'amazing']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PRINT VOCABULARY\n",
    "vocab =model.wv.index_to_key\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'learning':\n",
      "[-0.00053683  0.00023688  0.00510326  0.00900885 -0.00930313]...\n"
     ]
    }
   ],
   "source": [
    "# Get vector for a word\n",
    "vector = model.wv['learning']  # Returns 100-dim vector\n",
    "print(f\"Vector for 'learning':\\n{vector[:5]}...\")  # Show first 5 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words similar to 'learning':\n",
      "amazing: 0.219\n",
      "networks: 0.216\n",
      "machine: 0.093\n"
     ]
    }
   ],
   "source": [
    "# Find most similar words\n",
    "similar_words = model.wv.most_similar(\"learning\", topn=3)\n",
    "print(\"\\nWords similar to 'learning':\")\n",
    "for word, score in similar_words:\n",
    "    print(f\"{word}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1f86fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Download pre-trained model (~1.6GB)\n",
    "google_model = api.load('word2vec-google-news-300')\n",
    "\n",
    "# Example usage\n",
    "similar_words = google_model.most_similar(\"computer\", topn=3)\n",
    "print(\"\\nGoogle News model results for 'computer':\")\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c86840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get words and vectors\n",
    "words = [\"learning\", \"deep\", \"neural\", \"machine\", \"language\", \"processing\"]\n",
    "vectors = [model.wv[w] for w in words]\n",
    "\n",
    "# Reduce to 2D\n",
    "pca = PCA(n_components=2)\n",
    "vectors_2d = pca.fit_transform(vectors)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "for i, word in enumerate(words):\n",
    "    plt.scatter(vectors_2d[i,0], vectors_2d[i,1])\n",
    "    plt.annotate(word, (vectors_2d[i,0], vectors_2d[i,1]))\n",
    "plt.title(\"Word2Vec Visualization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d2fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def document_vector(model, doc):\n",
    "    # Remove out-of-vocabulary words\n",
    "    words = [w for w in doc if w in model.wv]\n",
    "    if len(words) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(model.wv[words], axis=0)\n",
    "\n",
    "# Convert all documents to vectors\n",
    "doc_vectors = [document_vector(model, doc) for doc in tokenized_sentences]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
